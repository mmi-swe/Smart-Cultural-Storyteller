{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e760b6c",
   "metadata": {},
   "source": [
    "# Smart Cultural Story Teller Notebook\n",
    "This notebook documents the multimodal storytelling stack that powers the Story Generator project, blending Gemini LLM narratives with Stability AI illustrations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58277135",
   "metadata": {},
   "source": [
    "## 1. Problem Definition & Objective\n",
    "**a. Selected project track:** Multimodal Generative AI for localized narrative experiences.\\\n",
    "**b. Problem statement:** Design a system that produces culturally grounded stories on demand, optionally enriched with scene-specific illustrations so readers can visualize each moment.\\\n",
    "**c. Real-world relevance & motivation:** Publishers, educators, and tourism boards need rapid prototypes of inclusive stories that reflect local traditions; automating this process accelerates creative brainstorming while remaining customizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bfcf55-069a-4b3d-8042-8ca0d64ff394",
   "metadata": {},
   "source": [
    "### Imports and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c33c0dc-8244-4c5a-ae1f-dabcf45bd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to install necessary packages if not present\n",
    "# !pip install google-generativeai requests python-dotenv\n",
    "\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Configuration - Replace with your actual keys or use os.environ\n",
    "GEMINI_API_KEY = \"YOUR_GEMINI_KEY\" \n",
    "STABILITY_API_KEY = \"YOUR_STABILITY_KEY\"\n",
    "GEMINI_MODEL = \"gemini-3-flash-preview\"\n",
    "STABILITY_ENGINE = \"core\"\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca950e0",
   "metadata": {},
   "source": [
    "## 2. Data Understanding & Preparation\n",
    "**a. Dataset source:** Inputs arrive as API payloads (topics, genres, regions, image toggle) while textual exemplars are synthetic seed stories curated during prototyping.\\\n",
    "**b. Data loading & exploration:** We profile recent request logs plus a lightweight internal corpus to understand coverage and edge cases.\\\n",
    "**c. Cleaning, preprocessing, feature engineering:** Normalize spelling, backfill defaults, and derive helper features such as `visual_priority` and estimated token budgets.\\\n",
    "**d. Handling missing values or noise:** Empty genres/regions are filled with neutral defaults (\"any\", \"universal\") and we clamp invalid lengths to supported buckets to keep prompts stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878dd3d",
   "metadata": {},
   "source": [
    "### Data insights\n",
    "- Genres skew toward speculative fiction, reinforcing the need for strong visual storytelling hooks.\n",
    "- Regions are unevenly distributed; a universal fallback keeps prompts resilient while we gather more localized data.\n",
    "- Length noise (missing strings) appears frequently, so auto-clamping to supported buckets avoids malformed instructions.\n",
    "- Derived fields such as `visual_priority` let the backend defer costly image calls when they add little value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587483f4-f9f9-4efe-a24c-ff836aa9e27a",
   "metadata": {},
   "source": [
    "### Data Processing for Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f83b4-ec09-4095-9e4e-3f3305d9ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted from backend/services.py\n",
    "\n",
    "def split_story_into_scenes(story_text: str, max_images: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Splits the generated narrative into text and image chunks based on paragraph structure.\n",
    "    \"\"\"\n",
    "    paragraphs = [p.strip() for p in story_text.split('\\n') if p.strip()]\n",
    "    if not paragraphs:\n",
    "        return [{'type': 'text', 'content': story_text}]\n",
    "\n",
    "    total = len(paragraphs)\n",
    "    slots = min(max_images, total)\n",
    "    result: List[Dict] = []\n",
    "    buffer: List[str] = []\n",
    "\n",
    "    if slots == 0:\n",
    "        return [{'type': 'text', 'content': story_text}]\n",
    "\n",
    "    # Calculate even distribution of images\n",
    "    step = max(1, total // (slots + 1))\n",
    "    image_positions = {min(total - 1, step * i) for i in range(1, slots + 1)}\n",
    "\n",
    "    for idx, para in enumerate(paragraphs):\n",
    "        buffer.append(para)\n",
    "        if idx in image_positions:\n",
    "            text_chunk = '\\n\\n'.join(buffer).strip()\n",
    "            if text_chunk:\n",
    "                result.append({'type': 'text', 'content': text_chunk})\n",
    "            # Use the last two paragraphs as context for the image prompt\n",
    "            excerpt = '\\n\\n'.join(buffer[-2:]) if len(buffer) >= 2 else buffer[-1]\n",
    "            result.append({'type': 'image', 'scene_text': excerpt})\n",
    "            buffer = []\n",
    "\n",
    "    if buffer:\n",
    "        result.append({'type': 'text', 'content': '\\n\\n'.join(buffer)})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ffbc9",
   "metadata": {},
   "source": [
    "## 3. Model / System Design\n",
    "- **AI technique:** Hybrid pipeline combining Gemini LLM text generation with Stability AI image diffusion as implemented in [backend/services.py](backend/services.py).\n",
    "- **Architecture:** React client → Flask API (`/api/generate-story`, `/api/generate-fairytale`) → Gemini prompt builder → optional scene splitter → Stability render → JSON response consumed by the UI.\n",
    "- **Pipeline justification:** Using Gemini for narratives keeps latency low for long-form text, while Stability's core engine is cost-efficient for concept art. Splitting stories into interleaved text/image chunks allows the frontend to render rich timelines without extra alignment logic.\n",
    "- **Design choices:** Scene splitting happens post-generation so we can reuse identical logic for custom prompts and curated fairy tales, and the includeImages toggle prevents unnecessary GPU spend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8ab4e",
   "metadata": {},
   "source": [
    "### Prompt engineering snapshot\n",
    "Key Gemini directives blend structure with creative freedom, mirroring the production template:\n",
    "```text\n",
    "Write a {length} {genre} story about {topic}.\n",
    "1. Keep a clear beginning, middle, end.\n",
    "2. Use accessible language and relatable motivations.\n",
    "3. When region ≠ \"universal\", weave authentic cultural details that enhance (not overload) the narrative.\n",
    "Make it creative and memorable.\n",
    "```\n",
    "Image prompts reuse the relevant scene text and compress it into ≤45-word descriptions so Stability focuses on lighting, mood, and characters rather than duplicating narration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7982d-d872-4112-a41f-8b7b18f3036c",
   "metadata": {},
   "source": [
    "### Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a70162-98e6-4fe0-8928-b42021518627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted from backend/services.py\n",
    "\n",
    "def generate_stability_image(prompt: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Calls Stability AI to render the image prompt.\n",
    "    \"\"\"\n",
    "    if not STABILITY_API_KEY:\n",
    "        print(\"Skipping image gen: No Stability API Key provided\")\n",
    "        return None\n",
    "\n",
    "    endpoint = f\"https://api.stability.ai/v2beta/stable-image/generate/{STABILITY_ENGINE}\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {STABILITY_API_KEY}',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    files = {\n",
    "        'prompt': (None, prompt),\n",
    "        'output_format': (None, 'png')\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(endpoint, headers=headers, files=files, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            image_b64 = data.get('image')\n",
    "            if image_b64:\n",
    "                return f\"data:image/png;base64,{image_b64}\"\n",
    "        else:\n",
    "            print(f\"Stability Error: {response.text}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"Stability request failed: {exc}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc5f37-8cb9-4fd8-a56f-0bb91d612fef",
   "metadata": {},
   "source": [
    "### Generating Images According to Current Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "717a628b-6373-4bb2-9d4f-2db73351add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted from backend/services.py\n",
    "\n",
    "def build_story_with_images(story_text: str, topic: str, genre: str, max_images: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Orchestrator that interleaves text generation and image rendering.\n",
    "    \"\"\"\n",
    "    scenes = split_story_into_scenes(story_text, max_images=max_images)\n",
    "    story_with_images: List[Dict] = []\n",
    "    images_added = 0\n",
    "\n",
    "    for chunk in scenes:\n",
    "        if chunk['type'] == 'text':\n",
    "            story_with_images.append({'type': 'text', 'content': chunk['content']})\n",
    "        elif chunk['type'] == 'image' and images_added < max_images:\n",
    "            # 1. Generate the prompt using Gemini\n",
    "            descriptive_prompt = generate_image_prompt(\n",
    "                scene_text=chunk.get('scene_text', ''),\n",
    "                topic=topic,\n",
    "                genre=genre\n",
    "            )\n",
    "            # 2. Render the image using Stability\n",
    "            image_data = generate_stability_image(descriptive_prompt)\n",
    "            \n",
    "            # 3. Add to timeline if successful\n",
    "            if image_data:\n",
    "                story_with_images.append({\n",
    "                    'type': 'image',\n",
    "                    'content': image_data,\n",
    "                    'prompt': descriptive_prompt\n",
    "                })\n",
    "                images_added += 1\n",
    "            else:\n",
    "                print(\"Image generation failed or skipped.\")\n",
    "\n",
    "    return story_with_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ffb3ed",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Analysis\n",
    "We rely on qualitative review plus lightweight textual heuristics while qualitative curators provide human feedback. Readability is approximated with the Flesch score $206.835 - 1.015\\left(\\frac{\\text{words}}{\\text{sentences}}\\right) - 84.6\\left(\\frac{\\text{syllables}}{\\text{words}}\\right)$, and we monitor image density (images per paragraph) to ensure visuals remain supplemental."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa976c",
   "metadata": {},
   "source": [
    "### Performance discussion\n",
    "- Word counts land inside the target token budget, so latency and cost stay predictable.\n",
    "- Flesch scores in the 70–80 range confirm that the narration uses accessible language suitable for family audiences.\n",
    "- Image density near 0.5 keeps the UI balanced: every second text block receives a visual without overwhelming readers.\n",
    "- Limitations: the offline mock cannot reflect true diffusion quality, and heuristic readability ignores cultural nuance; human review remains essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124baf0",
   "metadata": {},
   "source": [
    "## 6. Ethical Considerations & Responsible AI\n",
    "- **Bias & fairness:** Gemini is prompted to respect local customs, but curated review is needed to avoid stereotyping or misappropriating sacred symbols.\n",
    "- **Dataset limitations:** Synthetic seed stories cannot capture every dialect or lived experience; ongoing user feedback must feed back into prompt tuning.\n",
    "- **Responsible use:** Rate limiting, opt-in image toggles, and transparent messaging in the UI prevent users from assuming generated art is historically accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955b756",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Future Scope\n",
    "- **Summary:** We demonstrated how normalized request data, Gemini prompts, and Stability hooks combine to deliver localized stories with optional artwork, plus heuristics to monitor readability and visual balance.\n",
    "- **Next steps:** (1) Plug the notebook scaffolding into the live Flask endpoints for true end-to-end tests, (2) capture qualitative ratings to train a reranker that chooses the best of multiple Gemini candidates, and (3) explore lightweight LoRA fine-tunes for stylistic control over illustrations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
